# ğŸ¤– Mini Project #2 - Fundamentals of Intelligent Systems  

This repository contains **Mini Project #2** for the **Fundamentals of Intelligent Systems** course by **Amir Mohammad Saffar**, under the supervision of **Dr. Aliyar Shorehdeli**.

## ğŸ“Œ Project Overview  

This project explores various machine learning concepts, including **neural networks, activation functions, optimization algorithms, and feature engineering**. It is structured into multiple sections:

1ï¸âƒ£ **Comparison of Activation Functions** ğŸ”¥  
2ï¸âƒ£ **Feature Correlation Analysis** ğŸ“Š  
3ï¸âƒ£ **Impact of Hyperparameters in Neural Networks** ğŸ›ï¸  
4ï¸âƒ£ **Binary Image Conversion & Noise Handling** ğŸ–¼ï¸  
5ï¸âƒ£ **Reconstructing Missing Data using Hamming Networks** ğŸ”„  
6ï¸âƒ£ **Radial Basis Function (RBF) Neural Networks** ğŸŒ  

---

## ğŸ”¥ Question 1: Activation Function Analysis  

This section investigates how different activation functions impact neural network performance.

### ğŸ† Key Comparisons  
- **ReLU (Rectified Linear Unit)** âš¡  
  - Fast and widely used, but may suffer from **Dead Neurons** ğŸš«.  
- **ELU (Exponential Linear Unit)** ğŸ“ˆ  
  - Addresses the **Dead Neurons** issue by allowing small negative outputs.  

### âœ… Findings  
- **ELU performed better than ReLU** for handling negative inputs.  
- ReLUâ€™s **zero gradient issue** could hinder learning in deep networks.  

---

## ğŸ“Š Question 2: Feature Correlation Analysis  

This section explores the relationship between **tenure, education**, and other customer-related features.

### ğŸ” Insights  
- A **heatmap analysis** showed **high correlation** between **tenure & education** ğŸ«.  
- Histograms were used to visualize distributions for key features ğŸ“‰.  

---

## ğŸ›ï¸ Question 3: Impact of Hyperparameters  

Experiments with **different hyperparameters** for optimizing neural networks.

### ğŸš€ Techniques Used  
- **Batch Normalization** ğŸ“: Improved model convergence.  
- **Dropout** ğŸ›‘: Prevented overfitting, but impact was limited.  
- **L2 Regularization** ğŸ”’: Reduced model complexity without significant accuracy change.  
- **Optimizers Tested** âš™ï¸:  
  - **Adam** â 40.5% accuracy ğŸ“ˆ  
  - **RMSprop** â 41.9% accuracy ğŸš€  
  - **Ensemble Model** â 37% accuracy âŒ  

### ğŸ† Best Model  
ğŸ”¹ **RMSprop achieved the highest accuracy (41.9%)** on the validation dataset.  

---

## ğŸ–¼ï¸ Question 4: Binary Image Conversion & Noise Handling  

This section converts images into **binary format** and **adds noise** to study its effect.  

### ğŸ“Œ Methods Implemented  
- **convertImageToBinary()** ğŸ“·: Converts images to binary form (black & white).  
- **getNoisyBinaryImage()** ğŸ”Š: Adds **random noise** to test robustness.  

### âš ï¸ Key Observations  
- Adding **too much noise reduced model accuracy** significantly.  
- **Interpolation & Autoencoders** could help recover missing pixels.  

---

## ğŸ”„ Question 5: Reconstructing Missing Data using Hamming Networks  

**Hamming Networks** were used to reconstruct missing image data.

### ğŸ§ Observations  
- **30-50% missing pixels** â Model accuracy remains stable (~50%).  
- **Above 50% missing data** â Model struggles to reconstruct images âŒ.  
- **Techniques like interpolation & autoencoders can enhance performance**.  

---

## ğŸŒ Question 6: Radial Basis Function (RBF) Neural Networks  

A **Radial Basis Function (RBF) network** was implemented for **California Housing Price Prediction** ğŸ .

### ğŸ” Implementation Steps  
1ï¸âƒ£ **Load and preprocess the dataset** ğŸ“‚.  
2ï¸âƒ£ **Define an RBF layer** with Gaussian basis functions.  
3ï¸âƒ£ **Compare RBF vs. Dense networks** for regression.  

### ğŸ“Š Key Results  
- **RBF performed better in capturing non-linear patterns**.  
- **Dense networks worked well for large datasets with uniform distributions**.  

---

## ğŸš€ Future Improvements  
ğŸ”¹ **Hyperparameter Tuning** ğŸ›ï¸: Use Grid Search to optimize model parameters.  
ğŸ”¹ **Data Augmentation** ğŸ“ˆ: Generate more training data to improve performance.  
ğŸ”¹ **Exploring More Activation Functions** ğŸ¤–: Try Swish, SELU, and Leaky ReLU.  

---

## ğŸ“œ License  
This project is licensed under the **MIT License**.  

## ğŸ™ Acknowledgments  
Special thanks to **Scikit-Learn, TensorFlow, and Keras** for providing powerful tools to implement this project!  

---
