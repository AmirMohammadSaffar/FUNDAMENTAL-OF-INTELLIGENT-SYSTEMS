ğŸŒŸ Mini Project #1 - Fundamentals of Intelligent Systems

This project contains two main parts, corresponding to Questions 1 and 2 of the assignment for the Fundamentals of Intelligent Systems course.

ğŸ” Question 1: Predicting Customer Churn

In this part, we use a dataset related to customer information to predict which customers are likely to stop using the service.

ğŸ“Š Project Overview

Data Exploration ğŸ§: Analyze customer data such as age, income, and account details.

Data Preprocessing ğŸ› ï¸: Apply One-Hot Encoding to categorical features, handle missing values, and balance the dataset.

Machine Learning Models ğŸ¤–: Train different models, including Logistic Regression and Random Forest, to predict customer churn.

Handling Imbalanced Data âš–ï¸: Use SMOTE to balance the dataset and improve the model's ability to predict minority classes.

Model Evaluation ğŸ“ˆ: Compare model performance with metrics like Precision, Recall, F1-score, and ROC-AUC.

ğŸ“‚ Dataset

The dataset contains the following features:

Client Number ğŸ”¢: Unique identifier for each customer.

Attrition Flag ğŸš©: Indicates if the customer is continuing or has stopped services.

Demographic Information ğŸ‘¥: Age, gender, marital status, etc.

Account Details ğŸ’³: Credit limit, number of transactions, average utilization, etc.

ğŸ§  Key Concepts

Regularization ğŸ”’: We used L2 Regularization (Ridge) to prevent overfitting.

Imbalanced Data Handling âš–ï¸: Balanced the dataset with SMOTE to improve accuracy in predicting both classes.

Model Comparison âš™ï¸: Compared model performance before and after balancing the dataset.

ğŸ“Š Results

Model

Accuracy Before

Accuracy After

Logistic Regression

0.69

0.86

Random Forest

0.94

0.97

The results show improved performance in identifying customers likely to stop services after balancing the dataset.

ğŸ”„ Question 2: Polynomial Regression with Regularization

In this part, we explore polynomial regression with regularization techniques to reduce overfitting and improve model performance.

ğŸ“Š Project Overview

Polynomial Regression ğŸ“: Fit polynomial models with different degrees to see their impact on model accuracy.

Manual Regularization âœ‹ğŸ”’: Apply L2 regularization manually to prevent overfitting.

Model Comparison ğŸ¤–âš–ï¸: Compare different regression models (Linear, Ridge, Decision Tree).

ğŸ“‚ Dataset

The dataset is a simple one-dimensional array stored in data.npy.

Place the dataset in the appropriate location, such as /content/data.npy in Google Colab.

ğŸ§  Key Concepts Implemented

Polynomial Regression ğŸ“: Fit models of different degrees to observe their performance.

Manual Regularization âœ‹ğŸ”’: Implement L2 regularization manually to control model complexity.

Model Comparison âš™ï¸: Compare Linear Regression, Ridge Regression, and Decision Tree Regression.

ğŸ“Š Results and Observations

Training and Test Error ğŸ“‰: Compare errors for different models and polynomial degrees to understand overfitting.

Regularization Impact ğŸ”’: See how regularization helps in reducing overfitting.

ğŸš€ Future Improvements

Cross-Validation ğŸ”„: Add cross-validation to evaluate models more robustly.

Hyperparameter Tuning ğŸ›ï¸: Use Grid Search to find the best model parameters.

Explore Other Models ğŸŒ²: Try models like Random Forest or Support Vector Regression.

ğŸ“œ License

This project is licensed under the MIT License.

ğŸ™ Acknowledgments

Thanks to Scikit-Learn for providing the machine learning tools used in this project.

